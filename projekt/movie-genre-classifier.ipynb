{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zu welchem Genre gehört das Filmposter\n",
    "\n",
    "Ablauf:\n",
    "\n",
    "1. Importieren der Bibliotheken\n",
    "2. Laden der CSV Datei\n",
    "3. Erste Datenbereinigung\n",
    "4. Herunterladen der Film Poster\n",
    "5. Erstellen einer neuen CSV Datei\n",
    "6. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importieren der benötigen Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import numpy as npgc\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "import shutil\n",
    "import multiprocessing\n",
    "import urllib.error\n",
    "import urllib.request\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from time import time\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Laden der CSV Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "movie_data = pd.read_csv(\"./data/MovieGenre.csv\",\n",
    "                          sep=\",\", encoding='unicode_escape', index_col=None)\n",
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erste Datenbereinigung\n",
    "Löschen der Zeilen die einen Null Wert enthalten, da wir mit diesen Zeilen nichts anfangen können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Entfernen von Null Werten\n",
    "movie_data.dropna(subset=['imdbId', 'Genre', 'Poster'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Herunterladen der Film Poster (Parallel)\n",
    "Die Filmposter, die in der CSV Datei angegeben sind werden Heruntergeladen, dies passiert parallel damit dies schneller geht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_parallel(movies, image_dir):\n",
    "    # Create list of filenames\n",
    "    filenames = movies['imdbId'].apply(lambda imbdId : os.path.join(image_dir, str(imbdId)+'.jpg'))\n",
    "    # Create list of image urls\n",
    "    urls = movies['Poster']\n",
    "\n",
    "    # Create destination directory\n",
    "    if os.path.exists(image_dir):\n",
    "        print(\"Directory '{}' already exists and will be deleted.\".format(image_dir))\n",
    "        shutil.rmtree(image_dir)\n",
    "    print(\"Created new directory '{}'\".format(image_dir))\n",
    "    os.makedirs(image_dir)\n",
    "\n",
    "    # Define function to download one single image\n",
    "    def download_image(url, filename):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            return 0\n",
    "        except:\n",
    "            return os.path.basename(filename).split('.')[0]\n",
    "\n",
    "    # Download images in parallel\n",
    "    start = time()\n",
    "    print(\"\\nDownloading...\")\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    ko_list = Parallel(n_jobs=num_cores)(delayed(download_image)(u, f) for f, u in zip(filenames, urls))\n",
    "\n",
    "    print(\"\\nDownload in parallel mode took %d seconds.\" %(time()-start))\n",
    "    print(\"Success:\", len([i for i in ko_list if i==0]))\n",
    "    print(\"Errors:\", len([i for i in ko_list if i!=0]))\n",
    "\n",
    "    # Remove not downloaded posters from the dataframe\n",
    "    print(\"length of Movies before removing errors: \" + str(len(movies)))\n",
    "    imdbIdToBeRemoved = []\n",
    "    for item in ko_list:\n",
    "        if(item !=0):\n",
    "            imdbIdToBeRemoved.append(item)\n",
    "    movies= movies[~ movies[\"imdbId\"].astype(str).isin(imdbIdToBeRemoved)]\n",
    "    print(\"length of Movies after removing errors: \" + str(len(movies)))\n",
    "\n",
    "    return movies\n",
    "\n",
    "\n",
    "destination = './data/movie-posters'\n",
    "movie_data = download_parallel(movie_data, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstellen einer neuen CSV Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "movie_data.to_csv(os.path.join(data_dir, \"movies.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zweite Datenbereinigung\n",
    " 1. Wir zählen, wie oft ein Label vorkommt.\n",
    " 2. Speichern alle Labels die weniger als 1000 mal vorkommen in einer Liste\n",
    " 3. Entfernen aller Labels die mit einem der Labels in der zuvor erstellten Liste übereinstimmen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "movie_data = pd.read_csv(\"./data/movies.csv\")\n",
    "\n",
    "label_freq = movie_data['Genre'].apply(lambda s: str(s).split('|')).explode().value_counts().sort_values(ascending=False)\n",
    "rare = list(label_freq[label_freq<1000].index)\n",
    "\n",
    "display(rare)\n",
    "# Display what genres do we have and how many do we have\n",
    "# evtl Dramen rauschmeißen\n",
    "print(\"Number of movie posters in last download: {}\\n\".format(len(movie_data)))\n",
    "movie_data['Genre'] = movie_data['Genre'].apply(lambda s: [l for l in str(s).split('|') if l not in rare])\n",
    "\n",
    "movie_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Input Pipeline erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(movie_data['imdbId'], movie_data['Genre'], test_size=0.2, random_state=44)\n",
    "print(\"Number of posters for training: \", len(X_train))\n",
    "print(\"Number of posters for validation: \", len(X_val))\n",
    "\n",
    "# we need files instead of imdbIds\n",
    "X_train = [os.path.join('./data/movie-posters/', str(f)+'.jpg') for f in X_train]\n",
    "X_val = [os.path.join('./data/movie-posters/', str(f)+'.jpg') for f in X_val]\n",
    "print(X_train[:3])\n",
    "print(X_val[:3])\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_val = list(y_val)\n",
    "print(y_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Beispiel Bilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nobs = 8 # Maximum number of images to display\n",
    "ncols = 4 # Number of columns in display\n",
    "nrows = nobs//ncols # Number of rows in display\n",
    "\n",
    "style.use(\"default\")\n",
    "plt.figure(figsize=(12,4*nrows))\n",
    "for i in range(nrows*ncols):\n",
    "    ax = plt.subplot(nrows, ncols, i+1)\n",
    "    plt.imshow(Image.open(X_train[i]))\n",
    "    plt.title(y_train[i], size=10)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the multi-label binarizer on the training set\n",
    "print(\"Labels:\")\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(y_train)\n",
    "\n",
    "# Loop over all labels and show them\n",
    "N_LABELS = len(mlb.classes_)\n",
    "for (i, label) in enumerate(mlb.classes_):\n",
    "    print(\"{}. {}\".format(i, label))\n",
    "\n",
    "# transform the targets of the training and test sets\n",
    "y_train_bin = mlb.transform(y_train)\n",
    "y_val_bin = mlb.transform(y_val)\n",
    "\n",
    "# Print example of movie posters and their binary targets\n",
    "for i in range(3):\n",
    "    print(X_train[i], y_train_bin[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224 # Height and Width of an Image\n",
    "CHANNELS = 3 # 3 Stands for RGB\n",
    "# 224*224*3\n",
    "\n",
    "BATCH_SIZE = 256 # Number of learning repetitions\n",
    "SHUFFLE_BUFFER_SIZE = 1024 # Shuffle the training data by a chunck of 1024 observations\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE # Adapt preprocessing and prefetching dynamically\n",
    "\n",
    "\n",
    "def parse_function(filename, label):\n",
    "    # Read an image from a file\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    # Decode it into a dense vector\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=CHANNELS)\n",
    "    # Resize it to fixed shape\n",
    "    image_resized = tf.image.resize(image_decoded, [IMG_SIZE, IMG_SIZE])\n",
    "    # Normalize it from [0, 255] to [0.0, 1.0]\n",
    "    image_normalized = image_resized / 255.0\n",
    "    return image_normalized, label\n",
    "\n",
    "def create_dataset(filenames, labels, is_training=True):\n",
    "    # Create a first dataset of file paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    # Parse and preprocess observations in parallel\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if is_training == True:\n",
    "        # This is a small dataset, only load it once, and keep it in memory.\n",
    "        dataset = dataset.cache()\n",
    "        # Shuffle the data each buffer size\n",
    "        dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "    # Batch the data for multiple steps\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # Fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_ds = create_dataset(X_train, y_train_bin)\n",
    "val_ds = create_dataset(X_val, y_val_bin)\n",
    "\n",
    "for f, l in train_ds.take(1):\n",
    "    print(\"Shape of features array:\", f.numpy().shape)\n",
    "    print(\"Shape of labels array:\", l.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_url = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\"\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n",
    "                                         input_shape=(IMG_SIZE,IMG_SIZE,CHANNELS))\n",
    "                                         #mobilenet vorstellen\n",
    "feature_extractor_layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.kreas.Sequential([\n",
    "        feature_extractor_layer,\n",
    "        layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "        layers.Dense(N_LABELS, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model bauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "demoModel = tf.keras.Sequential([\n",
    "    layers.Conv2D(16,(3,3),activation='relu', input_shape=(IMG_SIZE,IMG_SIZE,3)),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "    \n",
    "    layers.Conv2D(16,(3,3),activation='relu'),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "    \n",
    "    layers.Conv2D(64,(3,3),activation='relu'),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "    \n",
    "    layers.Conv2D(64,(3,3),activation='relu'),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "    \n",
    "    layers.Conv2D(128,(3,3),activation='relu'),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "    \n",
    "    layers.Conv2D(128,(3,3),activation='relu'),\n",
    "    layers.MaxPool2D((2,2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    \n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(N_LABELS, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"complex_layers\"\n",
    "with open(f\"./data/models/{model_name}.txt\",'w') as fh:\n",
    "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "loss = tf.keras.metrics.binary_crossentropy\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "EPOCHS = 30\n",
    "start = time()\n",
    "history = model.fit(train_ds,epochs=EPOCHS,validation_data=create_dataset(X_val, y_val_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot to see how good our KI is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, EPOCHS), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, EPOCHS), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, EPOCHS), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, EPOCHS), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy on Meme Classification\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(f\"./data/models/{model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./data/models\"\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "model.save(f\"{model_dir}/{model_name}.h5\",save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Testing Images and Prepare Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_user_images(filename, label):\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=CHANNELS)\n",
    "    image_resized = tf.image.resize(image_decoded, [IMG_SIZE, IMG_SIZE])\n",
    "    image_normalized = image_resized / 255.0\n",
    "    image_normalized = np.expand_dims(image_normalized, axis= 0)\n",
    "    return image_normalized, label\n",
    "\n",
    "moviesTopPredict = []\n",
    "avengers, avengersLabels = prepare_user_images(\"./data/validationImages/Avengers-Action-Science-Fiction.jpg\",[\"Action\",\"Science-Fiction\"])\n",
    "crazyStupidLove, crazyStupidLoveLabels = prepare_user_images(\"./data/validationImages/crazyStupidLove-Romanze-Komödie.jpg\",[\"Romanze\", \"Komödie\"])\n",
    "peterchensMondfahrt, peterchensMondfahrtLabels = prepare_user_images(\"./data/validationImages/peterchensMondfahrt-Komödie-Fantasy-Abenteuer-Animation-Familienfilm.jpg\",[\"Komödie\", \"Fantasy\", \"Abenteuer\", \"Animation\", \"Familienfilm\"])\n",
    "#Demo begleiten (Handout):\n",
    "# Schritte con Datenbereinigung angefangen\n",
    "# abbildung Nuronales Netz\n",
    "# begriff erklären\n",
    "\n",
    "moviesTopPredict.append((avengers, avengersLabels))\n",
    "moviesTopPredict.append((crazyStupidLove, crazyStupidLoveLabels))\n",
    "moviesTopPredict.append((peterchensMondfahrt, peterchensMondfahrtLabels))\n",
    "movieNames = [\"Avengers\", \"Crazy Stupid Love\", \"Peterchens Mondfahrt\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(f\"./data/models/{model_name}.h5\")\n",
    "\n",
    "\n",
    "for index, tuple in enumerate(moviesTopPredict):\n",
    "    display(\"--------------\"+ movieNames[index] +\"------------------\")\n",
    "    prediction = (model.predict(tuple[0]) > 0.5).astype('int')\n",
    "    prediction = pd.Series(prediction[0])\n",
    "    prediction.index = mlb.classes_\n",
    "    prediction = prediction[prediction==1].index.values\n",
    "\n",
    "\n",
    "    display(\"Prediction: \" + str(list(prediction)))\n",
    "    display(\"Actual: \" + str(list(tuple[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1aa311f504caac0f188f78406e816835344f2fcf8805d2b336528ed71bf52c28"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
