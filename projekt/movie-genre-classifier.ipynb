{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zu welchem Genre geh√∂rt das Filmposter\n",
    "\n",
    "Ablauf:\n",
    "\n",
    "1. Importieren von allen Bibliotheken\n",
    "2. Einlesen der csv Datei\n",
    "3. Datenbereinigung\n",
    "4. Herunterladen der Bilder\n",
    "5. Input Pipeline erstellen\n",
    "6. Ki Model bauen\n",
    "7. Model trainieren und evaluieren\n",
    "8. Exportieren & sichern des Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import shutil\n",
    "import multiprocessing\n",
    "import urllib.error\n",
    "import urllib.request\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from time import time\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Laden der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "movie_data = pd.read_csv(\"./data/MovieGenre.csv\",\n",
    "                          sep=\",\", encoding='unicode_escape', index_col=None)\n",
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Datenbereinigung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Entfernen von Null Werten\n",
    "movie_data.dropna(subset=['imdbId', 'Genre', 'Poster'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Herunterladen der Daten (Parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def download_parallel(movies, image_dir):\n",
    "    # Create list of filenames\n",
    "    filenames = movies['imdbId'].apply(lambda imbdId : os.path.join(image_dir, str(imbdId)+'.jpg'))\n",
    "    # Create list of image urls\n",
    "    urls = movies['Poster']\n",
    "\n",
    "    # Create destination directory\n",
    "    if os.path.exists(image_dir):\n",
    "        print(\"Directory '{}' already exists and will be deleted.\".format(image_dir))\n",
    "        shutil.rmtree(image_dir)\n",
    "    print(\"Created new directory '{}'\".format(image_dir))\n",
    "    os.makedirs(image_dir)\n",
    "\n",
    "    # Define function to download one single image\n",
    "    def download_image(url, filename):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "            return 0\n",
    "        except:\n",
    "            return os.path.basename(filename).split('.')[0]\n",
    "\n",
    "    # Download images in parallel\n",
    "    start = time()\n",
    "    print(\"\\nDownloading...\")\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    ko_list = Parallel(n_jobs=num_cores)(delayed(download_image)(u, f) for f, u in zip(filenames, urls))\n",
    "\n",
    "    print(\"\\nDownload in parallel mode took %d seconds.\" %(time()-start))\n",
    "    print(\"Success:\", len([i for i in ko_list if i==0]))\n",
    "    print(\"Errors:\", len([i for i in ko_list if i!=0]))\n",
    "\n",
    "    # Remove not downloaded posters from the dataframe\n",
    "    print(\"length of Movies before removng errors\" + len(movies))\n",
    "    imdbIdToBeRemoved = []\n",
    "    for item in ko_list:\n",
    "        if(item !=0):\n",
    "            imdbIdToBeRemoved.append(item)\n",
    "    movies= movies[~ movies[\"imdbId\"].astype(str).isin(imdbIdToBeRemoved)]\n",
    "    print(\"length of Movies after removng errors\" + len(movies))\n",
    "\n",
    "    return movies\n",
    "\n",
    "\n",
    "destination = './data/movie-posters'\n",
    "movie_data = download_parallel(movie_data, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "movie_data.to_csv(os.path.join(data_dir, \"movies.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "movie_data = pd.read_csv(\"./data/movies.csv\")\n",
    "print(\"Number of movie posters in last download: {}\\n\".format(len(movie_data)))\n",
    "movie_data['Genre'] = movie_data['Genre'].str.split(\"|\")\n",
    "movie_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Input Pipeline erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(movie_data['imdbId'], movie_data['Genre'], test_size=0.2, random_state=44)\n",
    "print(\"Number of posters for training: \", len(X_train))\n",
    "print(\"Number of posters for validation: \", len(X_val))\n",
    "\n",
    "# we need files instead of imdbIds\n",
    "X_train = [os.path.join('./data/movie-posters/', str(f)+'.jpg') for f in X_train]\n",
    "X_val = [os.path.join('./data/movie-posters/', str(f)+'.jpg') for f in X_val]\n",
    "print(X_train[:3])\n",
    "print(X_val[:3])\n",
    "\n",
    "y_train = list(y_train)\n",
    "y_val = list(y_val)\n",
    "print(y_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Beispiel Bilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nobs = 8 # Maximum number of images to display\n",
    "ncols = 4 # Number of columns in display\n",
    "nrows = nobs//ncols # Number of rows in display\n",
    "\n",
    "style.use(\"default\")\n",
    "plt.figure(figsize=(12,4*nrows))\n",
    "for i in range(nrows*ncols):\n",
    "    ax = plt.subplot(nrows, ncols, i+1)\n",
    "    plt.imshow(Image.open(X_train[i]))\n",
    "    plt.title(y_train[i], size=10)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the multi-label binarizer on the training set\n",
    "print(\"Labels:\")\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(y_train)\n",
    "\n",
    "# Loop over all labels and show them\n",
    "N_LABELS = len(mlb.classes_)\n",
    "for (i, label) in enumerate(mlb.classes_):\n",
    "    print(\"{}. {}\".format(i, label))\n",
    "\n",
    "# transform the targets of the training and test sets\n",
    "y_train_bin = mlb.transform(y_train)\n",
    "y_val_bin = mlb.transform(y_val)\n",
    "\n",
    "# Print example of movie posters and their binary targets\n",
    "for i in range(3):\n",
    "    print(X_train[i], y_train_bin[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224 # Height and Width of an Image\n",
    "CHANNELS = 3 # 3 Stands for RGB\n",
    "\n",
    "BATCH_SIZE = 256 # Number of learning repetitions\n",
    "SHUFFLE_BUFFER_SIZE = 1024 # Shuffle the training data by a chunck of 1024 observations\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE # Adapt preprocessing and prefetching dynamically\n",
    "\n",
    "\n",
    "def parse_function(filename, label):\n",
    "    # Read an image from a file\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    # Decode it into a dense vector\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=CHANNELS)\n",
    "    # Resize it to fixed shape\n",
    "    image_resized = tf.image.resize(image_decoded, [IMG_SIZE, IMG_SIZE])\n",
    "    # Normalize it from [0, 255] to [0.0, 1.0]\n",
    "    image_normalized = image_resized / 255.0\n",
    "    return image_normalized, label\n",
    "\n",
    "def create_dataset(filenames, labels, is_training=True):\n",
    "    # Create a first dataset of file paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    # Parse and preprocess observations in parallel\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if is_training == True:\n",
    "        # This is a small dataset, only load it once, and keep it in memory.\n",
    "        dataset = dataset.cache()\n",
    "        # Shuffle the data each buffer size\n",
    "        dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "    # Batch the data for multiple steps\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # Fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_ds = create_dataset(X_train, y_train_bin)\n",
    "val_ds = create_dataset(X_val, y_val_bin)\n",
    "\n",
    "for f, l in train_ds.take(1):\n",
    "    print(\"Shape of features array:\", f.numpy().shape)\n",
    "    print(\"Shape of labels array:\", l.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model bauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # TODO:########################################################\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256,256, 3)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1024, activation='relu', name='hidden_layer'),\n",
    "    layers.Dense(N_LABELS, activation='sigmoid', name='output')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.metrics.binary_crossentropy,\n",
    "    metrics=\"accuracy\")\n",
    "\n",
    "start = time()\n",
    "history = model.fit(train_ds,\n",
    "                            epochs=30,\n",
    "                            validation_data=create_dataset(X_val, y_val_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot to see how good our KI is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, BATCH_SIZE), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, BATCH_SIZE), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, BATCH_SIZE), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, BATCH_SIZE), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "\n",
    "plt.title(\"Training Loss and Accuracy on Meme Classification\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1aa311f504caac0f188f78406e816835344f2fcf8805d2b336528ed71bf52c28"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
